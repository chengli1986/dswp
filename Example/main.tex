%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document



% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{graphicx}
\title{\LARGE \bf
Performance Evaluation and Improvement for Decoupled Software Pipelining Implementation
}

\author{\parbox{2 in}{\centering Ke Dai\\
        85261155\\
        Department of ECE\\
        Faculty of Applied Science\\
        University of British Columbia\\
        Vancouver, Canada\\
        {\tt\small kedai@ece.ubc.ca}} 
        \hspace*{ 0.5 in}
		\parbox{2 in}{\centering Li Cheng\\
        68530147\\
        Department of ECE\\
        Faculty of Applied Science\\
        University of British Columbia\\
        Vancouver, Canada\\
        {\tt\small b5z9a@ece.ubc.ca}}
        \hspace*{ 0.5 in}
        \parbox{2 in}{ \centering Zhirui Gao\\
        68877142\\
        Department of ECE\\
        Faculty of Applied Science\\
        University of British Columbia\\
        Vancouver, Canada\\
        {\tt\small zhiruig@ece.ubc.ca}}
}

% \author{Huibert Kwakernaak$^{1}$ and Pradeep Misra$^{2}$% <-this % stops a space
% \thanks{*This work was not supported by any organization}% <-this % stops a space
% \thanks{$^{1}$H. Kwakernaak is with Faculty of Electrical Engineering, Mathematics and Computer Science,
%         University of Twente, 7500 AE Enschede, The Netherlands
%         {\tt\small h.kwakernaak at papercept.net}}%
% \thanks{$^{2}$P. Misra is with the Department of Electrical Engineering, Wright State University,
%         Dayton, OH 45435, USA
%         {\tt\small p.misra at ieee.org}}%
% }


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract} 

Software Pipelining is an scheduling optimization technique to reorder instructions within a loop in order to minimize latency and avoid processor stalls. As one of the variant of software pipelining, decoupled software pipelining (DSWP) offers an automatic way to exploit thread-level parallelism on thread-parallel architectures such as Chip-Multiprocessor (CMP) or Simultaneous Multithreading (SMT) to speed up loop execution that specifically involves recursive data structure (RDS) in general programs. In this project, we focus on improving a premature version of DSWP implemented in Low-Level Virtual Machine (LLVM). Our work consists of three stages: First, fixing implementation bugs and porting of existing premature DSWP framework to latest LLVM code base. Second, evaluating current implementation and identifying performance bottleneck and corresponding root causes. Third, improving DSWP performance based from related literature review. Our project goal is to bring the DSWP LLVM pass to become the official LLVM pass one step closer. - 
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
While compiler and micro-architectural techniques have been largely successful in improving program performance by exploiting instruction-level parallelism (ILP). Neither of them have been very successful in exploiting thread-level parallelism in modern hardware architectures such as Simultaneous Multithreading (SMT) or Chip-Multiprocessor (CMP). Among the optimization techniques, most of them indeed to target to enhance loop parallelization because the time spent in loop execution usually dominates total program execution time \cite{c11}. 

Software Pipelining\cite{c10} is one of techniques to speed up loop execution. It restructures the loop so that a faster execution rate is achieved in supplement that loop iteration is executed in overlapped fashion to increase parallelism. While traditional software pipelining algorithms such as Modulo Scheduling and Kernel Recognition \cite{c11} focuses on ILP specific parallelism, a relatively new approach, Decoupled Software Pipelining, offers an automatic approach to extract fine-grained pipeline parallelism lurking in loops in most applications \cite{c3}. In addition, DSWP also shows potential in parallelizing recursive data structure (RDS) traversal loops, such as trees and graphs, which is largely immune to ILP optimization due to variable latency of loop-carried dependence through a pointer-chasing load \cite{c12}. The algorithm statically splits a single-threaded sequential loop into multiple non-speculative threads. 

Since the algorithm is proposed, DSWP has been implemented by several research groups on a variety of compilers mostly at back-end. In this project, we choose to study one premature Low-Level Virtual Machine implementation of DSWP and to improve its usability and performance. Our project is divided into four phases
\begin{enumerate}
\item revive and port existing implementation to latest LLVM code base,
\item fix algorithmic bugs and evaluate its performance on selected benchmarks, \footnote{\cite{c2} implementation fails to run on SPEC2006 due to some limitations},
\item study and identify root causes of performance bottleneck with proposed/implemented solution
\item re-evaluate proposed improvement on selected benchmarks and compare them with baseline performance obtained in phase 3
\end{enumerate}

The goal of this project is to fix implementation related bugs and port it to latest LLVM releases (3.7.1). through studying the existing implementation and fully understanding the algorithm and mechanisms behind it. We also would like to extend the limitation of running larger programs so we can evaluate the implementation on SPEC2006. This requires completeness in algorithmic implementation and potential improvements involved in thread partitioning and synchronization. The ultimate goal is to make contribution to the LLVM DSWP implementation so it can be used in a wider audience.

Progress

\begin{itemize}
\item install native Ubuntu 12.04 LTS on MacBookPro6,2 (i.e. mid-2010 model)
\item upgrade GCC and G++ to 4.8
\item upgrade CMAKE to 2.8.12.2 just as the version in Ubuntu 14.04 LTS VM
\item install LLVM and CLANG 3.7.1
\item test on native Ubuntu and DSWP optimized version still hangs...
\item setup SSH server on the native Ubuntu so both other two team members can access to the machine to do debugging
\item need to do some serious debugging
\item setup SSH server at 96.49.30.110
\end{itemize}

\section{Related Work}
Software pipelining has been widely used in a number of scenario in compiler design for modern architecture. V.H. Allan et. al performed a quite complete survey on software pipelining algorithm, which can be generally considered as Modulo Scheduling and Kernel Recognition. However, both methods are targeted to instruction-level parallelism by loop reformation. Especially, Modulo Scheduling requires hardware support to achieve acceptable performance. 

J. Llosa et. al proposed a novel heuristic approach based on modulo scheduling, Swing Modulo Scheduling, that generates schedules that are near optimal in terms of initiation interval, register requirements and stage count. Based on \cite{c13}, Tanya Lattner \cite{c8} implemented swing modulo scheduling algorithm in LLVM and further improved it using multiple basic blocks in the form of superblocks to capture additional parallelism opportunities at code-generation stage. The implementation initially remained within LLVM code base; however, it was obsoleted in recent releases. In 2015 LLVM developers' meeting, Brendon Cahoon implemented SMS algorithm at back-end with a number of additions to original algorithm that demonstrated incremental improvements. However, the implementation only targets at Qualcomm Hexagon 32-bit multi-threaded VLIW microarchitectures, which limits its application in other platforms. A high-level LLVM implementation of SMS was proposed by R. Jordans and H. Corporaal in \cite{c9}, which utilized a number of existing LLVM analysis and transform passes to recreate the SMS algorithm at LLVM IR. However, the work failed to provide clear experimental setup and results for performance evaluation. 

On the other stream, R. Rangan et. al first propOSED the decoupled software pipelining technique \cite{c12} along with a low-latency communication structure, synchronization array, between the cores. The evaluation was done for a variety of processor (IA-64 VLIW processor model) configurations connected with a simulated hardware synchronization array model in Liberty Simulation Environment \cite{c14}. G. Ottoni \cite{c3} proposed the complete algorithm for automatic thread extraction using DSWP in order to exploit thread-level parallelism with increasing popularity in chip multi-processors (CMPs) and reduce programmers' burden parallelizing loop intensive applications. \cite{c3} implemented DSWP in the back-end of IMPACT compiler \cite{c4}. Other work based on initial DSWP paper by E. Raman et. al [5, 6] added speculative thread scheduling to DSWP and improved scalability of DSWP by applying DOALL to some stages of the DSWP pipeline known as PS-DSWP. Similarly, these implementations were all done as a back-end pass on the IMPACT compiler. A recent work \cite{c15} by J. Huang et. al proposed DSWP+ algorithm that recognizes DSWP can also be an enabling transformation for other loop parallelization techniques, including DOALL, LOCALWRITE, and SpecDOALL. However, its implementation was manually tuned code transformation with assistance of LLVM and GCC in code generation. Therefore, it is not practical in production compiler. 

Zhao et. al \cite{c2} first implemented an almost complete DSWP thread extraction framework in LLVM along with POSIX threads programming model. However, the implementation contains some critical bugs in realizing program dependence graph, code splitting, and synchronization mechanism; in addition, the implementation is incomplete according to original proposed algorithm and industrial standard benchmarks (e.g. SPEC2006) failed to run on their implementation. A later work \cite{c7} is also based on the first LLVM DSWP implementation but it made significant changes and introduced back slicing technique to improve its performance.


% \subsection{Selecting a Template (Heading 2)}

% First, confirm that you have the correct template for your paper size. This template has been tailored for output on the US-letter paper size. Please do not use it for A4 paper since the margin requirements for A4 papers may be different from Letter paper size.

% \subsection{Maintaining the Integrity of the Specifications}

% The template is used to format your paper and style the text. All margins, column widths, line spaces, and text fonts are prescribed; please do not alter them. You may note peculiarities. For example, the head margin in this template measures proportionately more than is customary. This measurement and others are deliberate, using specifications that anticipate your paper as one part of the entire proceedings, and not as an independent document. Please do not revise any of the current designations

\section{Problem To Solve}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ORIGINAL PROJECT PROPOSAL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The project problem can be decomposed into four steps in order to achieve the project goal. First, we plan to revive  The first step is to evaluate the performance of current DSWP implementation with Linux time command and industrial standard benchmarks, then to diagnose the bottleneck of pass performance overhead. The second phase involves identifying the causes of performance overhead, we would like to prioritize each cause, carry out related literature review, and eventually implement suggested algorithms to address each cause one by one. The literature review should search for optimal solution for given overhead cause within appropriate amount of implementation complexity.

The problem to be solved is how to complete the implementation of DSWP algorithm and to make it usable with latest LLVM 3.7.1 release. Once we have a working implementation, we plan to evaluate the performance of our the new LLVM version of DSWP implementation on selected benchmarks. Ideally and practically, we would like to improve the working implementation so that it can run on large programs, for example, SPEC2006, which would provide us full performance metric. The project is mostly implementation oriented given the proposed approach and algorithm have been clearly stated and evaluated in \cite{c3}. However, it becomes an open-ended research question how to improve current algorithm, for example, eliminate redundant thread synchronization or strong scheduling heuristics. These approaches will be presented during the course of the project progress. 

\section{The DSWP Algorithm}
The original work \cite{c3} proposed a detailed set of stages utilizing DSWP to extract thread parallelism in hot loop execution. DSWP partitions loop body into several stages distributed across several long-running threads that communicate in a pipelined manner via inter-core queues. DSWP's pipeline organization keeps dependence recurrences local to the assigned thread, thus avoiding communication latency on the critical path (the critical path typically refers to heavy memory operations in a loop execution, for example, a linked list pointer chase). DSWP operates in four steps \cite{c3}: A) building a PDG (\textit{Program Dependence Graph}) for the loop, B) grouping dependence cycles into SSC (\textit{Strongly-Connected Components} that from an acyclic graph, C) allocating each SSC to a thread (a.k.a code splitting), D) inserting produce and consumer operations to transmit data values in case of data dependence and branch condition for control dependence. Since these four steps form the core of DSWP and critical to our refined implementation, we would like to describe each steps followed by discussions in more details below.

% NOT SURE IF WE NEED THIS OR NOT, PUT IT AS COMMENTS FOR NOW
%The use of non-speculative and decoupled threads produced by DSWP can increase execution efficiency and provide significant latency tolerance. Furthermore, DSWP decreases inter-core communication and resource requirements, by which reduces the overhead of parallelism. 



\subsection{Building the Dependence Graph}
The first step in DSWP algorithm is to build a program dependence graph, where each vertex represents one instruction and each edge corresponds to dependence among instructions. Dependencies must  contain all data dependence such as loads, stores or memory locations and all control dependence such as conditional branch or switch instructions. Figure1 illustrates a dependence graph of original loop. The arcs for intra-iteration dependencies are represented with solid lines; Dashed lines represent inter-iteration (or loop-carried) dependencies. Data dependence arcs are annotated with the corresponding register holding the value. Control dependence arcs have no label. In this example, there are no memory dependencies. We will discuss these three kinds of dependencies later. Special nodes are included in the top (bottom) of the graph to represent loop live-in (live-out) registers.

\begin{figure}[h!]
\includegraphics[width=0.3\textwidth]{figure1.png}
\includegraphics[width=0.15\textwidth]{figure2.png}
\caption{Original code and dependence graph}
  \centering
\end{figure}

There are two extensions to traditional control dependencies which are necessary to the correctness of DSWP transformation.
\begin{itemize}
\item Loop-iteration Control Dependence: 
Since that queue is reused every iteration and the use of it can be quite different from one iteration to another. Hence, it is vital to make sure that values from different loop recurrences were delivered correctly. Loop-iteration control dependence is inserted into standard control dependencies to guarantee the correctness. In the implementation \cite{c3}, they peel the first iteration of the loop so that instructions afterwards are duplicated as this. They then compute standard control dependencies based on the peeled version of code.
\end{itemize}

\begin{itemize}
\item Conditional Control Dependence: 
There are some dependencies which may or may not occur. For this situation, conditional control dependence is added to communicate the condition under whether such dependencies will occur. For example, in Figure2, D is control dependent on B, and U is data dependent on D. 
When U and D are assigned to different partitions (i.e. different threads), U cannot know if the value that it depended on was modified in D. To ensure the correct data dependence, an edge was added from B to U (conditional control dependence) to communicate with consuming thread U that whether the value in D was updated or not.
\end{itemize}

\begin{figure}[h!]
\includegraphics[width=0.2\textwidth]{figure4.png}
	\centering
\caption{The example of need for conditional control dependence in standard CFG}
  \centering
\end{figure}

\subsection{Thread Partitioning}
The thread partitioning problem is the problem of choosing a valid partitioning that minimizes the total execution time of resulting code. After building the program dependence, the algorithm partitions nodes into strongly connected components (SCCs) and creates directed acyclic graph of them. Thus, nodes are groups of instructions, and edges indicate a dependence between instructions belonging to the two groups of nodes. In each SCC component, instructions collectively participate in one dependence cycle, so that DSWP requires all instructions in the same SCC component to remain in the same thread.

\subsection{Splitting the Code}
In this step, DSWP algorithm computes and splits relevant basic blocks of original code and assigns them to a partition. Instructions of blocks belonging to one partition are placed to a new created basic block for their partition, maintaining their original relative orders. With splitting, branch instructions are assigned to partitions directly (e.g. the instruction E in Figure3) or they can be duplicated to implement a control dependence entering partitions (e.g. instruction B and B' in Figure3). 

\begin{figure}[h!]
\includegraphics[width=0.5\textwidth]{figure3.png}
	\centering
\caption{Producer thread and consumer thread}
  \centering
\end{figure}

\subsection{Inserting the Flows}
The final step is to add appropriate \textit{produce} and \textit{consume} instruction pairs (called flows) to communicate data and control synchronization. \textit{Produce} and \textit{consume} instructions are used to send and receive values respectively. Each \textit{produce} and \textit{consume} instruction takes an operand that indicates a queue to operate on, and the queue is reused between iterations. Additionally, these instructions are matched in order, so that the compiler can rely on this property to correctly transform the code. There are three kinds of flows that can be created based on the dependence type. 
\begin{itemize}
\item Data dependence: a data value is transmitted.
\end{itemize}
\begin{itemize}
\item Control dependence: a flag indicating a branch direction is transmitted to a duplicated branch. The control dependence of instruction B in Figure1 implemented by using \textit{producer} and \textit{consumer} threads illustrating in Figure3.
\end{itemize}
\begin{itemize}
\item Memory dependence: no value is transmitted. The flows in the case are used to enforce operation orders.
\end{itemize}

With above four steps of DSWP algorithm, one assumes to be able to automatically extract thread-level parallelism from DOALL and, more effectively, DOCROSS loops. 

\section{Methodology and Implementation}
As mentioned in project proposal, we maintains the proposed methodology:
\begin{enumerate}
\item port existing DSWP implementation to latest LLVM release 3.7.1
\begin{itemize}
\item fix any implementation flaws or bugs
\end{itemize}
\item evaluate performance of revived implementation using Linux \textit{time}
\item identify root causes of performance bottlenecks in current implementation
\item conduct research on critical performance bottleneck and implementation of the solution
\item re-evaluate performance of improved implementation and compare the performance to implementation without improvement
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WANT TO KEEP THE FOLLOWING JUST IN CASE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2. Target and implement three possible areas for improving poor performance of current DSWP through related literature review 
% high overhead of queuing system 
% redundant inter-core synchronization
% weak scheduling heuristics 

We have been primarily focused on porting the implementation to LLVM 3.7.1) and fixing implementation flaws in existing framework. We were hoping to complete the porting work and get the performance evaluation result within two weeks. However, we have observed a variety segmentation faults and a number of crashes due to memory mishandling even on a very simple for loop program. We are going to discuss a number of issues we have been investigating during the course of porting and some of them were addressed by strictly implementing the algorithm in \cite{c3}.

\subsection{API Code Refactoring} 

Since the existing work \cite{c2} was implemented back in 2008, which was based on LLVM 2.2. There have been significant changes in APIs between 2.2 and 7.3.1. There are mainly three areas affected by API changes from LLVM community:

\begin{itemize}
\item restructuring of header files location 
\item API changes of member classes and constructors
\item obsoleting or new APIs and fields 
\end{itemize}

This work is considered the most straightforward one, which also took us the least amount to complete. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WE ARE NOT USING THE TABLE TO LIST OUT THE CHANGES FOR NOW
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table}[h]
% \caption{Comparison of different LLVM API versions}
% \label{table1}
% \begin{center}
% \begin{tabular}{|l||l||l|}
% \hline
%  & Old version & Latest version\\
% \hline
% Path of library files & llvm/.. & llvm/IR/..\\
% \hline
%  & AliasAnalysis::Location & MemoryLocation\\
% %\hline
% %Instruction Set Size & 64-bit\\
% \hline
% a & b & s\\
% \hline
% a & b & s\\
% \hline
% a & b & s\\
% \hline
% a & b & s\\
% \hline
% a & b & s\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}

\subsection{Algorithmic Flaws Fixing}
We have been continuously encountering segmentation faults and crashes when we tested optimized binary using DSWP. Throughout the repetitive process, we observed that the original work \cite{c2} has a number of implementation flaws in realizing the algorithm. This is huge contrast to our original thought that existing implementation is mostly ready for use. The only issue exists in the implementation choice made of simplified loop-carried control dependence algorithm. We further discuss some of the flaws we observed and fixed during the course of our porting work.

\subsubsection{Program Dependence Graph Building} 
% The first step of DSWP is to determine dependencies among instructions and build PDG. As we mentioned in previous section, three dependencies are mainly considered here. Data dependence is straight-forward in LLVM by following the use-define chains. Memory dependence was done using LLVM built-in MemoryDependenceAnalysis pass. The most complex concept in DSWP algorithm is control dependence because of loop-iteration control dependence. In order to capture these dependencies, we use the method proposed in \cite{c3}. Basically, we create two copies of the loop which represent the first and second iterations. The first copy indicates the peeled off iteration. We then add edges between two copies of loops when one instruction in the first loop recurrence is dependent on an instruction in the following iteration. After that, we do traditional control dependence analysis on the peeled graph and finally corresponding nodes in two loop copies are merged into one node. Note that PostDominatorTree has been used to perform the standard control dependency analysis in our project.

Program dependence contains three types: data, memory, and control. The first two are straightforward where not many changes have been made in original implementation. Traditional def-use chains are used to determine data dependence and LLVM's MemoryDependenceAnalysis pass is used to determine memory dependence. However, \cite{c2} uses a simplified control dependence that only checks each branch instruction within the loop to see if it goes back to loop header. However, this is far from sufficient in practice. In addition, DSWP requires more complex version of control dependence where "loop-carried" control dependence is also considered. We incorporated the "peeled" graph approach in \cite{c3} that creates two copies of the loop. The first copy indicates the peeled off iteration. The control flow edges are added between the two copies whenever an instruction in the loop is dependent on the instruction used in the following iteration of the loop. We used PostDominateTree to construct a post dominate tree and obtained the control dependence by walking through the post-dominator tree. 

\subsubsection{Thread Partitioning}
% In this step, all SCCs are extracted easily by depth-first search in \cite{c2}. The problem is that the speedup obtainable by DSWP is limited by the portion of work performed by the slowest SCCs. For instance, if one SCC costs 10 seconds and the other three cost 3 seconds each, the program will take at least ten seconds with overhead to finish, no matter how many threads you use. Hence, we use a heuristic for partitioning SCCs to threads. We first estimate the latency of each SCC by adding estimates for each instruction. Here we add constant amount of time for every function call. (This latency time can vary widely, so further work are needed to create a more precise time measurement for a given function in each component.) Subsequently, we create a queue with candidate SCCs whose predecessors have all been scheduled. We then simply schedule components in this queue in decreasing order of time cost. This optimization in thread partitioning can improve run time performance through elimination of unnecessary overheads caused by inefficient SCC scheduling.

Thread partitioning involves estimating the latency of each SSC that are extracted from PDG. The latency is machine dependent and was hard-coded as a constant in \cite{c2}. Accordingly, we looked up and updated the latency of each instruction suitable for our experimental platform. However, it can be implemented with existing LLVM's utilities to estimate the amount of time that a given function will take to refine the latency estimation. 

We also observed that \cite{c2} incorrectly scheduled components when any of their parents had been scheduled instead of waiting for all of their parents to complete first. This is considered as one of main pitfalls that affects DSWP performance. 

\subsubsection{Code Splitting} 
Code splitting stage copies components to their corresponding thread and modifies components to refer to in-loop values. Based on the previous work \cite{c2}, the components run in separate threads. All necessary input parameters are placed into an array and passed into the thread, which is not efficient. Therefore, we attempted a different approach where variables are passed by providing thread a pointer to a structure containing those arguments. Furthermore, \cite{c2} has many bugs in its implementation. A critical mistake is that phi nodes are not considered in their implementation.

\subsubsection{Flow Insertion and Synchronization}
% Flows are inserted for thread communication and synchronization. Following the method proposed in \cite{c3}, \cite{c2} implemented queue to achieve communication using pthread APIs. For data dependence, the values of registers are passed through queue among threads. For control and memory dependence, there are no actual value to pass. They push and pop empty values to enforce correct operation orders. However, their implementation contains several bugs. They simply inserted produce instruction after the variable was defined, and added consume instruction before the value would be used. In this situation, when this value was only used in several iterations, the producer and consumer threads would be out of synchronized quickly. So far we are still rewriting the code and try to fix the bugs. 

This stage inserts producer and consumer threads that can communicate with each other. A simple synchronization queue was implemented in \cite{c2}, which effectively serves the purpose regardless of its performance. However, in the original implementation, queue data is cast to 64-bit integers but in reality queue data is a single type. We dynamically create run-time queues for required structure data, which removes the overhead of casting and additional memory operations. 
We also observed deadlocks in \cite{c2} as the flow insertion is asymmetric (not come in pair), which would happen when the value is only used in some iterations. This is also a bad consequence from simplified loop control dependence. We made changes to the code to achieve conservative synchronization, which could be too extreme because many redundant queues are used to ensure full synchronization. This is something we could target to improve in the future. 

Up to now, we have addressed all segmentation faults and crash cases seen in original implementation. Moreover, we have more confidence in algorithmic implementation of revived implementation. We are more likely to spot further issues in evaluation that we need to come back to revisit, and we have count the possibility in part of remaining work. 

\section{Experimental Setup And Results}
We carried out all evaluations on the experimental platform shown in Table 1. The main testing environment is Ubuntu 14.04 LTS installed with LLVM 3.7.1 running as a Virtual Machine on a Macbook Pro host. 

\begin{table}[h]
\caption{Experimental Platform}
\label{table1}
\begin{center}
\begin{tabular}{|l||l|}
\hline
Host Machine & Apple Macbook Pro (Mid 2015)\\
\hline
Host Processor & Intel Core i7 2.5GHz\\
\hline
Host Configuration & 4 Physical Cores, 2 Threads per Core\\
%\hline
%Instruction Set Size & 64-bit\\
\hline
Host Memory & 16GB DDR3 SDRAM 1600MHz\\
\hline
Host OS & OSX El Capitan 10.11.4\\
\hline
VM Configuration & 8 Physical Cores, 1 Thread per Core\\
\hline
VM Memory & 8GB DDR3 SDRAM 1600MHz\\
\hline
VM OS & Ubuntu 14.04 LTS\\
\hline
\end{tabular}
\end{center}
\end{table}

We plan to evaluate DSWP firstly on simple loop structure to examine the correctness and then its performance. Up to now, with a number of fixing implemented, we are obtaining promising transformed LLVM IR file with \textit{produce} and \textit{consume} IR instructions inserted. In addition, We eyeballed the correctness of loop body partitioning from generated optimized IR. However, we are not yet successfully to get the DSWP optimized binary to run to complete. The observed behaviour is program hang after producer and consumer threads are created. We were suspecting the issue existing in either flow insertion stage or queuing system. We then realized that our virtual machine host is not configured with HW threading enabled for each core. Technically, each core still runs single thread. This observation defeats the purpose of DSWP completely. We plan to run our code on a native Ubuntu machine that has HW multi-threading enabled properly. 

% Before you begin to format your paper, first write and save the content as a separate text file. Keep your text and graphic files separate until after the text has been formatted and styled. Do not use hard tabs, and limit use of hard returns to only one return at the end of a paragraph. Do not add any kind of pagination anywhere in the paper. Do not number text heads-the template will do that for you.

% Finally, complete content and organizational editing before formatting. Please take note of the following items when proofreading spelling and grammar:

% \subsection{Abbreviations and Acronyms} Define abbreviations and acronyms the first time they are used in the text, even after they have been defined in the abstract. Abbreviations such as IEEE, SI, MKS, CGS, sc, dc, and rms do not have to be defined. Do not use abbreviations in the title or heads unless they are unavoidable.

% \subsection{Units}


% \subsection{Equations}

% The equations are an exception to the prescribed specifications of this template. You will need to determine whether or not your equation should be typed using either the Times New Roman or the Symbol font (please no other font). To create multileveled equations, it may be necessary to treat the equation as a graphic and insert it into the text after your paper is styled. Number equations consecutively. Equation numbers, within parentheses, are to position flush right, as in (1), using a right tab stop. To make your equations more compact, you may use the solidus ( / ), the exp function, or appropriate exponents. Italicize Roman symbols for quantities and variables, but not Greek symbols. Use a long dash rather than a hyphen for a minus sign. Punctuate equations with commas or periods when they are part of a sentence, as in

% $$
% \alpha + \beta = \chi \eqno{(1)}
% $$

% Note that the equation is centered using a center tab stop. Be sure that the symbols in your equation have been defined before or immediately following the equation. Use Ò(1)Ó, not ÒEq. (1)Ó or Òequation (1)Ó, except at the beginning of a sentence: ÒEquation (1) is . . .Ó

% \subsection{Some Common Mistakes}
% \begin{itemize}
% \item The word ÒdataÓ is plural, not singular.
% \item The subscript for the permeability of vacuum ?0, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ÒoÓ.
% \item In American English, commas, semi-/colons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
% \item A graph within a graph is an ÒinsetÓ, not an ÒinsertÓ. The word alternatively is preferred to the word ÒalternatelyÓ (unless you really mean something that alternates).
% \item Do not use the word ÒessentiallyÓ to mean ÒapproximatelyÓ or ÒeffectivelyÓ.
% \item In your paper title, if the words Òthat usesÓ can accurately replace the word ÒusingÓ, capitalize the ÒuÓ; if not, keep using lower-cased.
% \item Be aware of the different meanings of the homophones ÒaffectÓ and ÒeffectÓ, ÒcomplementÓ and ÒcomplimentÓ, ÒdiscreetÓ and ÒdiscreteÓ, ÒprincipalÓ and ÒprincipleÓ.
% \item Do not confuse ÒimplyÓ and ÒinferÓ.
% \item The prefix ÒnonÓ is not a word; it should be joined to the word it modifies, usually without a hyphen.
% \item There is no period after the ÒetÓ in the Latin abbreviation Òet al.Ó.
% \item The abbreviation Òi.e.Ó means Òthat isÓ, and the abbreviation Òe.g.Ó means Òfor exampleÓ.

% \end{itemize}


\section{Remaining Work}

We are still going to complete the evaluation of the revived implementation on selected benchmarks, and hopefully SPEC2006. Given the complexity of DSWP algorithm itself and engineering tradeoff in compiler IR layer, we are not going to improve all three possible areas, which are high overhead queuing system, redundant synchronization, and weak scheduling heuristic. Instead, we plan to pick the scheduling heuristic as our main target and plan to improve it using a stronger scheduling heuristic, possibly a speculative scheduling by \cite{c5} in order to enhance the performance of DSWP. While it's ideal to see improvement with a different heuristic implemented, our main goal is to produce stable and usable DSWP implementation, which is also the rule of thumb for implementing many production compiler optimization techniques. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THIS SECTION LEAVES TO FINAL REPORT 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Future work}
% 1. Profile and reproduce the performance metric of current DSWP implementation ideally on SPEC2006 benchmark, which has not been successfully run on DSWP 
% 2. Target and implement three possible areas for improving poor performance of current DSWP through related literature review:
% \begin{itemize}
% \item high overhead of queuing system 
% \end{itemize}

% \begin{itemize}
% \item redundant inter-core synchronization
% \end{itemize}

% \begin{itemize}
% \item weak scheduling heuristics 
% \end{itemize}

% 3. Rerun the benchmark on modified version of DSWP and compare performance with the old version 

 

% Use this sample document as your LaTeX source file to create your document. Save this file as {\bf root.tex}. You have to make sure to use the cls file that came with this distribution. If you use a different style file, you cannot expect to get required margins. Note also that when you are creating your out PDF file, the source file is only part of the equation. {\it Your \TeX\ $\rightarrow$ PDF filter determines the output file size. Even if you make all the specifications to output a letter file in the source - if you filter is set to produce A4, you will only get A4 output. }

% It is impossible to account for all possible situation, one would encounter using \TeX. If you are using multiple \TeX\ files you must make sure that the ``MAIN`` source file is called root.tex - this is particularly important if your conference is using PaperPlaza's built in \TeX\ to PDF conversion tool.

% \subsection{Headings, etc}

% Text heads organize the topics on a relational, hierarchical basis. For example, the paper title is the primary text head because all subsequent material relates and elaborates on this one topic. If there are two or more sub-topics, the next level head (uppercase Roman numerals) should be used and, conversely, if there are not at least two sub-topics, then no subheads should be introduced. Styles named ÒHeading 1Ó, ÒHeading 2Ó, ÒHeading 3Ó, and ÒHeading 4Ó are prescribed.

% \subsection{Figures and Tables}

% Positioning \cite{c1} Figures and Tables: Place figures and tables at the top and bottom of columns. Avoid placing them in the middle of columns. Large figures and tables may span across both columns. Figure captions should be below the figures; table heads should appear above the tables. Insert figures and tables after they are cited in the text. Use the abbreviation ÒFig. 1Ó, even at the beginning of a sentence.

% \begin{table}[h]
% \caption{An Example of a Table}
% \label{table_example}
% \begin{center}
% \begin{tabular}{|c||c|}
% \hline
% One & Two\\
% \hline
% Three & Four\\
% \hline
% \end{tabular}
% \end{center}
% \end{table}


%    \begin{figure}[thpb]
%       \centering
%       \framebox{\parbox{3in}{We suggest that you use a text box to insert a graphic (which is ideally a 300 dpi TIFF or EPS file, with all fonts embedded) because, in an document, this method is somewhat more stable than directly inserting a picture.
% }}
%       %\includegraphics[scale=1.0]{figurefile}
%       \caption{Inductance of oscillation winding on amorphous
%        magnetic core versus DC bias magnetic field}
%       \label{figurelabel}
%    \end{figure}
   

% Figure Labels: Use 8 point Times New Roman for Figure labels. Use words rather than symbols or abbreviations when writing Figure axis labels to avoid confusing the reader. As an example, write the quantity ÒMagnetizationÓ, or ÒMagnetization, MÓ, not just ÒMÓ. If including units in the label, present them within parentheses. Do not label axes only with units. In the example, write ÒMagnetization (A/m)Ó or ÒMagnetization {A[m(1)]}Ó, not just ÒA/mÓ. Do not label axes with a ratio of quantities and units. For example, write ÒTemperature (K)Ó, not ÒTemperature/K.Ó

\section{Conclusion}

DSWP is a complete algorithm yet an efficient implementation has been developed in LLVM given the number of stages involved in the proposed algorithm and high complexity of each stage\cite{c3}. The project aims to make continuing contribution to DSWP implementation under LLVM so that it can achieve official acceptance by LLVM developer community from both functional and performance perspectives to allow general usage from wider audience. 

% \section{Team Member Contribution}
% Ke Dai - 20% contribution on literature review
% Li Cheng - 40% on 
% Zhirui Gao - 40%

\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section*{APPENDIX}

% Appendixes should appear before the acknowledgment.

% \section*{ACKNOWLEDGMENT}

% The preferred spelling of the word ÒacknowledgmentÓ in America is without an ÒeÓ after the ÒgÓ. Avoid the stilted expression, ÒOne of us (R. B. G.) thanks . . .Ó  Instead, try ÒR. B. G. thanksÓ. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.

% Note that Harvard citation is used from Google Scholar 
% to keep format consistency

\begin{thebibliography}{3}

\bibitem{c1} Lattner, C. and Adve, V., 2004, March. LLVM: A compilation framework for lifelong program analysis \& transformation. In \textit{Code Generation and Optimization, 2004. CGO 2004. International Symposium on} (pp. 75-86). IEEE.
\bibitem{c2} Zhao, F. and Hahnenberg, M., Implementation of decoupled software pipelining (DSWP) in the LLVM compiler infrastructure.
\bibitem{c3} Ottoni, G., Rangan, R., Stoler, A. and August, D.I., 2005, November. Automatic thread extraction with decoupled software pipelining. In \textit{Proceedings of the 38th annual IEEE/ACM International Symposium on Microarchitecture} (pp. 105-118). IEEE Computer Society.
\bibitem{c4} August, D.I., Connors, D.A., Mahlke, S.A., Sias, J.W., Crozier, K.M., Cheng, B.C., Eaton, P.R., Olaniran, Q.B. and Hwu, W.M.W., 1998, April. Integrated predicated and speculative execution in the IMPACT EPIC architecture. In \textit{ACM SIGARCH Computer Architecture News} (Vol. 26, No. 3, pp. 227-237). IEEE Computer Society.
\bibitem{c5} Vachharajani, N., Rangan, R., Raman, E., Bridges, M.J., Ottoni, G. and August, D.I., 2007, September. Speculative decoupled software pipelining. In \textit{Proceedings of the 16th International Conference on Parallel Architecture and Compilation Techniques} (pp. 49-59). IEEE Computer Society.
\bibitem{c6} Raman, E., Ottoni, G., Raman, A., Bridges, M.J. and August, D.I., 2008, April. Parallel-stage decoupled software pipelining. In \textit{Proceedings of the 6th annual IEEE/ACM international symposium on Code generation and optimization} (pp. 114-123). ACM.
\bibitem{c7} Alwan, E., Fitch, J. and Padget, J., 2015. Enhancing the performance of Decoupled Software Pipeline through Backward Slicing. arXiv preprint arXiv:1501.06743.
\bibitem{c8} Lattner, T.M., 2005. An implementation of swing modulo scheduling with extensions for superblocks (Doctoral dissertation, University of Illinois at Urbana-Champaign).
\bibitem{c9} Jordans, R. and Corporaal, H., 2015, June. High-level software-pipelining in LLVM. In \textit{Proceedings of the 18th International Workshop on Software and Compilers for Embedded Systems} (pp. 97-100). ACM.
\bibitem{c10} Lam, M., 1988, June. Software pipelining: An effective scheduling technique for VLIW machines. In \textit{ACM Sigplan Notices} (Vol. 23, No. 7, pp. 318-328). ACM.
\bibitem{c11} Allan, V.H., Jones, R.B., Lee, R.M. and Allan, S.J., 1995. Software pipelining. \textit{ACM Computing Surveys (CSUR), 27}(3), pp.367-432.
\bibitem{c12} Rangan, R., Vachharajani, N., Vachharajani, M. and August, D.I., 2004, September. Decoupled software pipelining with the synchronization array. In \textit{Proceedings of the 13th International Conference on Parallel Architectures and Compilation Techniques} (pp. 177-188). IEEE Computer Society.
\bibitem{c13} Llosa, J., González, A., Ayguadé, E. and Valero, M., 1996, October. Swing module scheduling: a lifetime-sensitive approach. In \textit{Parallel Architectures and Compilation Techniques, 1996., Proceedings of the 1996 Conference on} (pp. 80-86). IEEE.
\bibitem{c14} Vachharajani, M., Vachharajani, N., Penry, D.A., Blome, J.A. and August, D.I., 2002. Microarchitectural exploration with Liberty. In \textit{Microarchitecture, 2002.(MICRO-35). Proceedings. 35th Annual IEEE/ACM International Symposium on} (pp. 271-282). IEEE.
\bibitem{c15} Huang, J., Raman, A., Jablin, T.B., Zhang, Y., Hung, T.H. and August, D.I., 2010, April. Decoupled software pipelining creates parallelization opportunities. In \textit{Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and optimization} (pp. 121-130). ACM.
\end{thebibliography}
\end{document}
